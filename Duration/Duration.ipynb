{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ps/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_phonemes(durations):\n",
    "    \"\"\"returns max phoneme length and max number of frames\"\"\"\n",
    "    max_phonemes = 0\n",
    "    max_frames = 0\n",
    "    for i in durations:\n",
    "        max_phonemes = max(len(i),max_phonemes)\n",
    "        l = list(zip(*i)[1])\n",
    "        max_temp = 0\n",
    "        for temp in l:\n",
    "            max_temp = max(max_temp, temp)\n",
    "        max_frames =  max(max_temp, max_frames)\n",
    "    return max_frames, max_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Set of hyperparameters\"\"\"\n",
    "with open(\"duration_pred.pkl\",'r') as f:\n",
    "    durations,speaker_dur = pickle.load(f)\n",
    "    \n",
    "max_phoneme_frames, max_num_phonemes = get_max_phonemes(durations)   #get from data... Setting to 10 for example only\n",
    "batch_size = 32\n",
    "speak_feats = 16 #features in speaker embedding\n",
    "phoneme_feats = 16\n",
    "\n",
    "num_time_buckets = 30 \n",
    "\n",
    "phoneme_count = 40   #take form list\n",
    "\n",
    "num_speakers = 108  #Get from file \n",
    "\n",
    "mlp_layers = 4 #number of layers in MLP\n",
    "mlp_units = 256 #number of units in each MLP layer\n",
    "\n",
    "num_rnn_layers = 4  #number of layers in RNN network\n",
    "num_hidden_gru = 512 #number of hidden units in each RNN layer\n",
    "\n",
    "with open(\"max_frames_speakers_count.pkl\") as f:\n",
    "    max_frames, speakers, count = pickle.load(f)\n",
    "\n",
    "#phonemes\n",
    "silence = \"SIL\"\n",
    "\n",
    "phonemes = [silence,'AA','AE','AH','AO','AW','AY','B','CH','D','DH','EH',\n",
    "            'ER','EY','F','G','HH','IH','IY','JH','K','L','M','N','NG','OW','OY',\n",
    "            'P','R','S','SH','T','TH','UH','UW','V','W','Y','Z','ZH']\n",
    "\n",
    "phoneme_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [batch_size, max_num_phonemes,phoneme_count]) #Phonemes whose duration is to be predicted, one phoeneme at a time\n",
    "\n",
    "y_inp = tf.placeholder(tf.int32, [batch_size,max_num_phonemes])  #ground truth BUCKETED durations of phonemes obtained \n",
    "                                                                   #from segmentation model. Stored as indices from\n",
    "                                                                   #0 to num_time_buckets - 1\n",
    "\n",
    "y_length = tf.placeholder(tf.int32, [batch_size]) #stores length of sequence of phonemes for the batch\n",
    "speaker_selector = tf.placeholder(tf.float32, [batch_size, num_speakers]) #One hot speaker selector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'speaker_embedding/MatMul:0' shape=(32, 16) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"16 feature speaker embedding for better distinction in speakers\"\"\"\n",
    "with tf.variable_scope(\"speaker_embedding\"):\n",
    "    speaker = tf.get_variable(\"speaker\",\n",
    "                              shape=(num_speakers,speak_feats),\n",
    "                              dtype= tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                              trainable=True)\n",
    "    speaker_matrix = tf.matmul(speaker_selector, speaker)\n",
    "speaker_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'phoneme_embedding/Reshape_1:0' shape=(32, 106, 16) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"16 feature phoneme embedding for better distinction in phonemes\"\"\"\n",
    "with tf.variable_scope(\"phoneme_embedding\"):\n",
    "    phoneme_emb = tf.get_variable(\"phoneme_emb\",\n",
    "                              shape=(len(phonemes),phoneme_feats),\n",
    "                              dtype= tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                              trainable=True)\n",
    "    unstacked_x = tf.reshape(x, [-1,len(phonemes)])\n",
    "    phoneme_array = tf.matmul(unstacked_x, phoneme_emb)\n",
    "    phoneme_matrix = tf.reshape(phoneme_array,[batch_size, max_num_phonemes,phoneme_feats])\n",
    "phoneme_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MLP_speaker/fully_connected/BiasAdd:0' shape=(32, 16) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"fully connected layer that takes speaker as the input and sends state to phonemes before MLP\"\"\"\n",
    "with tf.variable_scope(\"MLP_speaker\"):\n",
    "    mlp_speaker_outputs = phoneme_feats\n",
    "    mlp_speaker = tf.contrib.layers.fully_connected(speaker_matrix,\n",
    "                                                    mlp_speaker_outputs,\n",
    "                                                    activation_fn= None)\n",
    "mlp_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MLP_preprocessing/concat:0\", shape=(32, 106, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"appending phonemes to speaker embedding.\"\"\"\n",
    "with tf.variable_scope(\"MLP_preprocessing\"):\n",
    "    speaker_tiled = tf.contrib.seq2seq.tile_batch(tf.expand_dims(mlp_speaker,0), multiplier = max_num_phonemes)\n",
    "    speaker_tiled = tf.transpose(speaker_tiled, [1,0,2])\n",
    "    mlp_input = tf.concat([speaker_tiled,phoneme_matrix],axis=2)\n",
    "    \n",
    "print mlp_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MLP/fully_connected_3/BiasAdd:0' shape=(32, 106, 256) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"creating the MLP network which takes phoneme sequence appended to speaker embedding as input\"\"\"\n",
    "with tf.variable_scope(\"MLP\"):\n",
    "    mlp_output = []\n",
    "    MLP = [mlp_input,]  #index 0 is the input, index 1 to n correspond to layer 1 to n respectively\n",
    "    for i in range(mlp_layers):\n",
    "        layer = tf.contrib.layers.fully_connected(MLP[i],\n",
    "                                                  mlp_units,\n",
    "                                                  activation_fn=None)  ## which activation to use??? Default = relu\n",
    "                                                                             ## I have used tanh for now.\n",
    "        MLP.append(layer)\n",
    "    mlp_output = MLP[mlp_layers]\n",
    "\n",
    "mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn_speaker/fully_connected/BiasAdd:0' shape=(32, 512) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"fully connected layer that takes speaker as the input and sends state to the Bidirectional Gru layers\"\"\"\n",
    "with tf.variable_scope(\"rnn_speaker\"):\n",
    "    rnn_speaker_outputs = num_hidden_gru\n",
    "    rnn_speaker = tf.contrib.layers.fully_connected(speaker_matrix,\n",
    "                                                    rnn_speaker_outputs,\n",
    "                                                    activation_fn= None)\n",
    "rnn_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_BI4/concat:0' shape=(32, 106, 1024) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gru_cell():\n",
    "    return tf.nn.rnn_cell.GRUCell(num_hidden_gru)\n",
    "\n",
    "inputs = tf.nn.dropout(mlp_output, keep_prob = 0.85)\n",
    "\n",
    "\"\"\"creating the BiDirectional GRU layers\"\"\"\n",
    "\n",
    "for i in range(num_rnn_layers):\n",
    "    with tf.variable_scope(\"RNN_BI\"+str(i+1)):\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(gru_cell(),\n",
    "                                                                 gru_cell(),\n",
    "                                                                 inputs,\n",
    "                                                                 sequence_length=y_length,\n",
    "                                                                 initial_state_fw = rnn_speaker,\n",
    "                                                                 initial_state_bw = rnn_speaker)\n",
    "\n",
    "        inputs = tf.concat(outputs, axis = 2)\n",
    "\n",
    "rnn_output = inputs\n",
    "rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'CRF_preprocessing/fully_connected/Reshape_1:0' shape=(32, 106, 30) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Creating FC layer to reduce RNN output dimension to the number of Time Buckets. SOftmax to convert to probabilities \"\"\"\n",
    "with tf.variable_scope(\"CRF_preprocessing\"):\n",
    "    crf_input = tf.contrib.layers.fully_connected(rnn_output,\n",
    "                                                    num_time_buckets,\n",
    "                                                    activation_fn= tf.nn.softmax)\n",
    "    \n",
    "crf_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CRF Implementation\"\"\"\n",
    "with tf.variable_scope(\"CRF\"):\n",
    "    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(crf_input,\n",
    "                                                          y_inp, \n",
    "                                                          y_length)\n",
    "    crf_loss = tf.reduce_mean(-log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Learning rate and annealing rate are set for the adam optimizer\"\"\"\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "starter_learning_rate = 6e-4\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, \n",
    "                                           global_step,\n",
    "                                           400,\n",
    "                                           0.9,\n",
    "                                           staircase=True)\n",
    "\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "minimize = optimizer.minimize(crf_loss,global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Decode_Durations/ReverseSequence_1:0' shape=(32, 106) dtype=int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"CRF Decoder to get bucketed durations\"\"\"\n",
    "with tf.variable_scope(\"Decode_Durations\"):\n",
    "    vetrebi_sequence, vetrebi_score = tf.contrib.crf.crf_decode(crf_input,\n",
    "                                                                transition_params,\n",
    "                                                                y_length)\n",
    "vetrebi_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_phonemes(a, phonemes):\n",
    "    \"\"\"One hot encoding of a phoneme\"\"\"\n",
    "    ans = np.zeros((len(phonemes)))\n",
    "    ans[phonemes.index(a)] = 1\n",
    "    return ans\n",
    "        \n",
    "def get_phonemes_encoded(durations,phonemes):\n",
    "    \"\"\"Returns one hot encoding of all phonemes in a sentence \"\"\"\n",
    "    encoded_phonemes = []\n",
    "    for i in durations:\n",
    "        phoneme_list = []\n",
    "        for j in i:\n",
    "            phoneme_list.append(encode_phonemes(j[0],phonemes))\n",
    "        for j in range(max_num_phonemes - len(i)):\n",
    "            phoneme_list.append(encode_phonemes(phonemes[0],phonemes))  ##silence\n",
    "        encoded_phonemes.append(phoneme_list)\n",
    "    \n",
    "    return np.array(encoded_phonemes)                         \n",
    "                            \n",
    "    \n",
    "def get_durations_in_buckets(durations):\n",
    "    \"\"\"Algorithm to convert durations to buckets\"\"\"\n",
    "    ans = []\n",
    "    min_frame_len = 0.01 #### 10ms \n",
    "    asn_upper = np.log(.95 * max_phoneme_frames)\n",
    "    asn_lower = np.log(min_frame_len)\n",
    "    inc = (asn_upper - asn_lower)/(num_time_buckets-2)\n",
    "\n",
    "    for i in durations:\n",
    "        i = list(zip(*i)[1])\n",
    "        i = [math.ceil((np.log(x)-asn_lower)/inc) for x in i]\n",
    "        for j in range(max_num_phonemes - len(i)):\n",
    "            i.append(0)  ##silence\n",
    "        ans.append(i)\n",
    "    return np.array(ans)  \n",
    "      \n",
    "    \n",
    "def get_durations(buckets):\n",
    "    \"\"\"Returns durations predicted from buckets\"\"\"\n",
    "    ans = []\n",
    "    min_frame_len = 0.01 #### 10ms \n",
    "    asn_upper = np.log(.95 * max_phoneme_frames)\n",
    "    asn_lower = np.log(min_frame_len)\n",
    "    inc = (asn_upper - asn_lower)/(num_time_buckets-2)\n",
    "\n",
    "    for i in buckets:\n",
    "        ans.append(np.ceil(np.e**((i*inc)+asn_lower)*10))\n",
    "\n",
    "    return np.array(ans)  \n",
    "\n",
    "\n",
    "def get_true_durations(frames):\n",
    "    \"\"\"Converts frames to ms\"\"\"\n",
    "    return np.array(frames)*10\n",
    " \n",
    "\n",
    "def get_length(durations):\n",
    "    \"\"\"returns the number of phonemes in all sentences\"\"\"\n",
    "    seq_lengths = []\n",
    "    for i in durations:\n",
    "        seq_lengths.append(len(i))\n",
    "    return np.array(seq_lengths)\n",
    "\n",
    "def get_speaker_one_hot(speaker_list, speakers):\n",
    "    \"\"\"returns one hot encoding of speakers\"\"\"\n",
    "    one_hot_speaker = np.zeros((len(speaker_list),len(speakers)))\n",
    "    for i in range(len(speaker_list)):\n",
    "        one_hot_speaker[i][speakers.index(speaker_list[i])]=1\n",
    "    return one_hot_speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOS(test_durations, test_speaker):\n",
    "    \"\"\"Returns Mean abolute Error in ms\"\"\"\n",
    "    ptr=0\n",
    "    no_of_batches=len(test_durations)/batch_size\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for j in range(no_of_batches):\n",
    "        inp_durations = test_durations[ptr:ptr+batch_size]\n",
    "        inp_speakers = list(zip(*test_speaker[ptr:ptr+batch_size])[0])\n",
    "\n",
    "        x_test = get_phonemes_encoded(inp_durations,phonemes)\n",
    "\n",
    "        y_inp_test = get_durations_in_buckets(inp_durations)\n",
    "\n",
    "        y_length_test = get_length(inp_durations)\n",
    "\n",
    "        speaker_selector_test = get_speaker_one_hot(inp_speakers, speakers)\n",
    "\n",
    "        ptr+=batch_size\n",
    "    \n",
    "        pred = sess.run(vetrebi_sequence,{x:x_test, y_length:y_length_test,speaker_selector:speaker_selector_test})\n",
    "        for i in range(batch_size):\n",
    "            pred_temp = get_durations(pred[i])[:y_length_test[i]-1]\n",
    "            true_temp = get_true_durations(list(zip(*inp_durations[i])[1]))[:-1]\n",
    "            error_temp = np.abs(np.subtract(true_temp,pred_temp))\n",
    "            error += np.sum(error_temp)\n",
    "            total += y_length_test[i]\n",
    "    print inp_durations[0]\n",
    "    print get_durations(sess.run(vetrebi_sequence,\n",
    "                       {x:x_test, y_length:y_length_test,speaker_selector:speaker_selector_test})[0])[:y_length_test[0]-1]\n",
    "    print get_true_durations(list(zip(*inp_durations[0])[1]))[:-1]\n",
    "    print(\"MOS:\")\n",
    "    return float(error)/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize session\"\"\"\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_models/epoch36/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For using previous models, restore, else comment\"\"\"\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"saved_models/epoch36/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For Training only- set number of epochs. Calculates Mean Absolute Error at every epoch\\n    Uncomment for training'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"For Training only- set number of epochs. Calculates Mean Absolute Error at every epoch\n",
    "    Uncomment for training\"\"\"\n",
    "\n",
    "# %%time\n",
    "# epoch = 100\n",
    "# cur_error=0\n",
    "# min_error=100\n",
    "\n",
    "\n",
    "# train_durations, test_durations, train_speaker, test_speaker = train_test_split(durations,speaker_dur, test_size=0.1, random_state=20)\n",
    "# no_of_batches=len(train_durations)/batch_size\n",
    "\n",
    "# for i in range(epoch):\n",
    "\n",
    "#     ptr=0\n",
    "#     for j in range(no_of_batches):\n",
    "#         if j%100 == 0:\n",
    "#             print j\n",
    "#         inp_durations = train_durations[ptr:ptr+batch_size]\n",
    "\n",
    "#         inp_speakers = list(zip(*train_speaker[ptr:ptr+batch_size])[0])\n",
    "\n",
    "#         x_train = get_phonemes_encoded(inp_durations,phonemes)\n",
    "\n",
    "#         y_inp_train = get_durations_in_buckets(inp_durations)\n",
    "\n",
    "#         y_length_train = get_length(inp_durations)\n",
    "\n",
    "#         speaker_selector_train = get_speaker_one_hot(inp_speakers, speakers)\n",
    "\n",
    "#         ptr+=batch_size\n",
    "#         sess.run(minimize,{x:x_train, y_inp:y_inp_train, y_length:y_length_train,speaker_selector:speaker_selector_train})\n",
    "#     print \"Epoch - \",str(i)\n",
    "\n",
    "#     print sess.run(crf_loss,{x:x_train, y_inp:y_inp_train, y_length:y_length_train,speaker_selector:speaker_selector_train})\n",
    "    \n",
    "#     cur_error = MOS(test_durations[:10*batch_size], test_speaker[:10*batch_size])\n",
    "\n",
    "#     print \"ERROR- \",str(cur_error)\n",
    "#     if cur_error < min_error:\n",
    "# #         save_path = saver.save(sess,\"saved_models/epoch\"+str(i)+\"/model.ckpt\")\n",
    "# #         print \"Saved @ \", save_path\n",
    "#         min_error = cur_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 41.  14. 124.  28. 124.  14.  41.  86.  14.  41.  14.  41.  86.  86.\n",
      "  14.  41.  86.  86.  28.  41.  86.  28. 124.  28.  41. 124.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14. 124.  28. 124.  14.  41.  86.  14.  41.  14.  41.  86. 124.\n",
      "  14.  41.  86.  86.  28.  41.  86.  28. 124.  28.  41. 124.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14. 124.  28. 124.  14.  41.  86.  14.  41.  14.  41.  86.  86.\n",
      "  14.  41.  86.  86.  28.  41.  86.  28. 124.  28.  86.  14.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14. 124.  28. 124.  14.  41.  86.  14.  41.  14.  41.  14. 124.\n",
      "  14.  41.  86.  86.  28.  41.  86.  28. 124.  28.  86.  14.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14. 124.  28. 124.  14.  41.  86.  14.  41.  14.  41.  86.  86.\n",
      "  14.  41.  86.  86.  28.  41.  86.  28. 124.  28.  41. 124.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14. 124.  28. 124.  14.  41.  86.  14.  86.  14.  86.  14. 124.\n",
      "  14.  41.  86.  86.  28.  41.  86.  28. 124.  28.  86.  14.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14.  86.  14. 124.  14.  28.  86.  14.  41.  14.  41.  86.  86.\n",
      "  14.  41.  86.  41.  28.  41.  86.  28. 124.  28.  41.  14.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14.  86.  28. 124.  14.  28.  86.  14.  41.  14.  41.  86.  86.\n",
      "  14.  41.  86.  41.  28.  41.  86.  28. 124.  28.  41.  14.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14. 124.  28. 124.  14.  41.  86.  14.  41.  14.  41.  86. 124.\n",
      "  14.  41.  86.  86.  28.  41.  86.  28. 124.  28.  41. 124.  86.  14.\n",
      " 124.  86.  14.]\n",
      "[ 41.  14. 124.  28. 124.  14.  41.  86.  14.  41.  14.  41.  86.  86.\n",
      "  14.  41.  86.  86.  28.  41.  86.  28. 124.  28.  86.  14.  86.  14.\n",
      " 124.  86.  14.]\n"
     ]
    }
   ],
   "source": [
    "def get_durations_of_phonemes(sentence, speaker_index):\n",
    "    \"\"\"Takes the phonemes of a sentence along with speaker index and returns predicted durations for every phoneme\"\"\"\n",
    "    sentence.append(silence)\n",
    "    temp = [[i,i] for i in sentence]\n",
    "    temp = [temp]*batch_size\n",
    "    temp = get_phonemes_encoded(temp,phonemes)\n",
    "    speaker_id = speakers[speaker_index]\n",
    "    speaker_temp = [speaker_id]*batch_size\n",
    "    speaker_temp = get_speaker_one_hot(speaker_temp,speakers)\n",
    "    length_temp = [len(sentence)]*batch_size\n",
    "    \n",
    "    print get_durations(sess.run(vetrebi_sequence,\n",
    "                       {x:temp, y_length:length_temp,speaker_selector:speaker_temp})[0])[:length_temp[0]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    get_durations_of_phonemes(['IH', 'T', 'SIL', 'T', 'ER', 'N', 'D', 'SIL', 'M', 'IY', 'SIL', 'AH', 'G', 'EH', 'N', 'S', 'T',\n",
    "                         'SIL', 'DH', 'AH', 'SIL', 'HH', 'OW', 'L', 'SIL', 'S', 'IH', 'S', 'T', 'AH', 'M'], i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
