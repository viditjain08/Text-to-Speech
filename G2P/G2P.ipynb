{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viditjain08/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.set_printoptions(threshold='nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set False for testing\"\"\"\n",
    "training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read CMUDict line by line and counted the maximum letters in a word and maximum phonemes in its conversion\"\"\"\n",
    "F = open(\"CMUDict\", \"r\")\n",
    "x_temp = []\n",
    "y_temp = []\n",
    "max_count_x = 0\n",
    "max_count_y = 0\n",
    "while 1:\n",
    "    temp = F.readline()\n",
    "    if not temp:\n",
    "        break\n",
    "    temp = temp.split()\n",
    "    if len(temp[0])>3 and temp[0][-3]==\"(\" and temp[0][-1]==\")\":\n",
    "        continue\n",
    "    if max_count_x<len(temp[0]):\n",
    "        max_count_x=len(temp[0])\n",
    "    if max_count_y<len(temp)-1:\n",
    "        max_count_y=len(temp)-1\n",
    "    x_temp.append(temp[0])\n",
    "    y_temp.append(temp[1:])\n",
    "F.close()\n",
    "max_count_x+=2\n",
    "max_count_y+=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Created a list of unique letters and phonemes along with their counts\"\"\"\n",
    "a = set()\n",
    "for i in x_temp:\n",
    "    for j in i:\n",
    "        a.add(j)\n",
    "alphabets = list(a)\n",
    "alpha_count = len(alphabets)\n",
    "alpha_count+=2\n",
    "phoneme_count=41\n",
    "\n",
    "phonemes = ['AA','AE','AH','AO','AW','AY','B','CH','D','DH','EH',\n",
    "            'ER','EY','F','G','HH','IH','IY','JH','K','L','M','N','NG','OW','OY',\n",
    "            'P','R','S','SH','T','TH','UH','UW','V','W','Y','Z','ZH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', \"'\", '-', '.', '1', '0', '3', '2', '5', '4', '7', '6', '9', '8', 'A', 'C', 'B', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'J', 'M', 'L', 'O', 'N', 'Q', 'P', 'S', 'R', 'U', 'T', 'W', 'V', 'Y', 'X', 'Z', '\\\\', '_', 'end']\n",
      "['start', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH', 'end']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Include start and end for both letters and phonemes\"\"\"\n",
    "alphabets.insert(0,\"start\")\n",
    "alphabets.append(\"end\")\n",
    "phonemes.insert(0,\"start\")\n",
    "phonemes.append(\"end\")\n",
    "print alphabets\n",
    "print phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"converts words to one hot encoding with end padding to make size uniform\"\"\"\n",
    "x = np.zeros((len(x_temp),max_count_x,alpha_count))\n",
    "for i in range(len(x_temp)):\n",
    "    x[i][0][0]=1\n",
    "    for j in range(max_count_x-1):\n",
    "        if j < len(x_temp[i]):\n",
    "            x[i][j+1][alphabets.index(x_temp[i][j])]=1\n",
    "        else:\n",
    "            x[i][j+1][alpha_count-1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"converts phonemes to one hot encoding with end padding to make size uniform\"\"\"\n",
    "y = np.zeros((len(y_temp),max_count_x,phoneme_count))\n",
    "for i in range(len(y_temp)):\n",
    "    y[i][0][0]=1\n",
    "    for j in range(max_count_x-1):\n",
    "        if j < len(y_temp[i]):\n",
    "            t = re.sub(r'[0-9]+', '', y_temp[i][j])\n",
    "            y[i][j+1][phonemes.index(t)]=1\n",
    "        else:\n",
    "            y[i][j+1][phoneme_count-1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_x(l):\n",
    "    \"\"\"convert one hot encoding of words to human readable form\"\"\"\n",
    "    l1 = []\n",
    "    for i in range(len(l)):\n",
    "        l1.append(alphabets[list(l[i]).index(1)])\n",
    "    return l1\n",
    "def convert_y(l):\n",
    "    \"\"\"convert one hot encoding of phonemes to human readable form\"\"\"\n",
    "    l1 = []\n",
    "    for i in range(len(l)):\n",
    "        l1.append(phonemes[np.argmax(l[i])])\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "x_train = tf.placeholder(tf.float32, [batch_size, max_count_x,alpha_count])\n",
    "y_train = tf.placeholder(tf.float32, [batch_size, max_count_x,phoneme_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set of hyperparameters\"\"\"\n",
    "num_hidden = 1024\n",
    "num_layers = 3\n",
    "output_keep_prob = 0.95\n",
    "beam_width = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_cell():\n",
    "    \"\"\"Returns a new GRU Cell\"\"\"\n",
    "    return tf.nn.rnn_cell.GRUCell(num_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'encoder_gru/rnn/while/Exit_3:0' shape=(64, 1024) dtype=float32>,\n",
       " <tf.Tensor 'encoder_gru/rnn/while/Exit_4:0' shape=(64, 1024) dtype=float32>,\n",
       " <tf.Tensor 'encoder_gru/rnn/while/Exit_5:0' shape=(64, 1024) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoder\n",
    "\"\"\"Creates a multi layer Unidirectional RNN encoder with dropout\"\"\"\n",
    "with tf.variable_scope(\"encoder_gru\"):\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_y = gru_cell()\n",
    "        cell_y = tf.nn.rnn_cell.DropoutWrapper(cell_y, output_keep_prob=output_keep_prob)\n",
    "        cells.append(cell_y)\n",
    "    encoder_cell =  tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True) \n",
    "    #initial_state = cell1.zero_state(batch_size, dtype=tf.float32)\n",
    "    encoder_val, encoder_state = tf.nn.dynamic_rnn(encoder_cell, x_train, dtype=tf.float32)\n",
    "encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoder object at 0x7f0315bca790>\n",
      "BasicDecoderOutput(rnn_output=<tf.Tensor 'decoder_gru/decoder/transpose:0' shape=(64, ?, 1024) dtype=float32>, sample_id=<tf.Tensor 'decoder_gru/decoder/transpose_1:0' shape=(64, ?) dtype=int32>)\n",
      "Tensor(\"decoder_gru/decoder/transpose:0\", shape=(64, ?, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Creates a multi layer Unidirectional RNN decoder with dropout and initial state from last hidden state of encoder\n",
    "    along with a Dense layer at the end\"\"\"\n",
    "with tf.variable_scope(\"decoder_gru\"):\n",
    "    decoder_cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_x = gru_cell()\n",
    "        cell_x = tf.nn.rnn_cell.DropoutWrapper(cell_x, output_keep_prob=output_keep_prob)\n",
    "        decoder_cells.append(cell_x)\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cells)\n",
    "    # TRAINING DECODER\n",
    "    p_layer = tf.layers.Dense(phoneme_count, use_bias=True, kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = y_train,sequence_length = [max_count_x],time_major = False)\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell = decoder_cell,\n",
    "        helper = training_helper,\n",
    "        #output_layer = p_layer,\n",
    "        initial_state = encoder_state)\n",
    "    print training_decoder\n",
    "    training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder = training_decoder)\n",
    "    print training_decoder_output\n",
    "    training_logits = training_decoder_output.rnn_output\n",
    "    print training_logits\n",
    "    training_logits = p_layer(training_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transpose:0\", shape=(?, 64, 41), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "val = tf.transpose(training_logits, [1, 0, 2])\n",
    "print val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Softmax is applied to the outputs to get phoneme probabilities at each time step\n",
    "    and a start is added in the start to each output\"\"\"\n",
    "val = tf.reshape(val, [-1,phoneme_count])\n",
    "prediction = tf.nn.softmax(val)\n",
    "prediction = tf.reshape(prediction, [-1,batch_size,int(y_train.get_shape()[2])])\n",
    "temp_arr = np.zeros((batch_size, phoneme_count))\n",
    "temp_arr[:,0]=1\n",
    "prediction = tf.concat([tf.expand_dims(tf.constant(temp_arr, dtype=tf.float32),0),prediction], axis=0)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'beam_search/tile_batch/Reshape:0' shape=(320, 1024) dtype=float32>, <tf.Tensor 'beam_search/tile_batch/Reshape_1:0' shape=(320, 1024) dtype=float32>, <tf.Tensor 'beam_search/tile_batch/Reshape_2:0' shape=(320, 1024) dtype=float32>)\n",
      "Tensor(\"beam_search/Const:0\", shape=(41, 41), dtype=float32)\n",
      "FinalBeamDecoderOutput(predicted_ids=<tf.Tensor 'beam_search/decoder/transpose:0' shape=(64, ?, 5) dtype=int32>, beam_search_decoder_output=BeamSearchDecoderOutput(scores=<tf.Tensor 'beam_search/decoder/transpose_1:0' shape=(64, ?, 5) dtype=float32>, predicted_ids=<tf.Tensor 'beam_search/decoder/transpose_2:0' shape=(64, ?, 5) dtype=int32>, parent_ids=<tf.Tensor 'beam_search/decoder/transpose_3:0' shape=(64, ?, 5) dtype=int32>))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Beam search implemented for predicting phonemes from a word\"\"\"\n",
    "with tf.variable_scope(\"beam_search\"):\n",
    "    decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    print decoder_initial_state\n",
    "    temp = np.zeros((phoneme_count, phoneme_count))\n",
    "    for i in range(phoneme_count):\n",
    "        temp[i][i]=1\n",
    "    decoder_embedding = tf.constant(temp, dtype=tf.float32)\n",
    "    print decoder_embedding\n",
    "    # PREDICTING_DECODER\n",
    "    predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell = decoder_cell,\n",
    "        embedding = decoder_embedding,\n",
    "        start_tokens = tf.fill([batch_size], 0),\n",
    "        end_token = 40,\n",
    "        initial_state = decoder_initial_state,\n",
    "        output_layer = p_layer,\n",
    "        beam_width = beam_width)\n",
    "    predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder = predicting_decoder,\n",
    "        maximum_iterations = 2*max_count_x)\n",
    "    print predicting_decoder_output\n",
    "    predicting_logits = predicting_decoder_output.predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cross entropy of decoded outputs and true phonemes\"\"\"\n",
    "y_t1 = tf.transpose(y_train, [1,0,2])\n",
    "cross_entropy = -tf.reduce_sum(y_t1 * tf.log(tf.clip_by_value(prediction,1e-10,1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Learning rate and Annealing Rate is set for the Adam optimizer\"\"\"\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.85, staircase=True)\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "minimize = optimizer.minimize(cross_entropy,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To display tensorboard\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter( './logs/1/train ', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Session Initialization\"\"\"\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./8.2571937673/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For using previous models, restore, else comment\"\"\"\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"./8.2571937673/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_output, test_output = train_test_split(x,y, test_size=0.1, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(test_input, test_output):\n",
    "    cur_error=0\n",
    "    mistakes=0\n",
    "    total=0\n",
    "    test_batches=int(len(test_input)/batch_size)\n",
    "    test_new = np.argmax(test_output, axis=2)[:test_batches*batch_size]\n",
    "    p_test=0\n",
    "    for k in range(test_batches):\n",
    "        test_inp, test_out = test_input[p_test:p_test+batch_size], test_new[p_test:p_test+batch_size]\n",
    "        p_test+=batch_size\n",
    "        batch_test_new = sess.run(predicting_logits, {x_train: test_inp})[:,:,0]\n",
    "\n",
    "        for l in range(batch_size):\n",
    "            out = test_out[l]\n",
    "            out = out[1:list(out).index(40)]\n",
    "            predicted_out = np.pad(batch_test_new[l], (0,34), 'constant')\n",
    "            predicted_out = predicted_out[:len(out)]\n",
    "            mat = np.not_equal(out, predicted_out)\n",
    "            mistakes+=np.count_nonzero(mat)\n",
    "            total+=len(out)\n",
    "    return 100*float(mistakes)/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"For Training only- set number of epochs. Calculates error% at every epoch\"\"\"\n",
    "\n",
    "if training:\n",
    "\n",
    "    no_of_batches = int(len(train_input)/batch_size)\n",
    "    epoch = 60\n",
    "    prev_error=0\n",
    "    cur_error=0\n",
    "    min_error=100\n",
    "\n",
    "    for i in range(epoch):\n",
    "        ptr = 0\n",
    "        for j in range(no_of_batches):\n",
    "            inp, out = train_input[ptr:ptr+batch_size], train_output[ptr:ptr+batch_size]\n",
    "            ptr+=batch_size\n",
    "            sess.run(minimize,{x_train: inp, y_train: out})\n",
    "        print \"Epoch - \",str(i)\n",
    "\n",
    "        prev_error = cur_error\n",
    "        cur_error = get_error(test_input, test_output)\n",
    "        print \"Error % = \" + str(cur_error)\n",
    "        break\n",
    "        if cur_error<min_error:\n",
    "            save_path = saver.save(sess, \"./saved_model/model.ckpt\")\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "            min_error = cur_error\n",
    "        else:\n",
    "            print \"Not Saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing Function to convert word to one hot\"\"\"\n",
    "def convert_to_one_hot(word):\n",
    "    x_t = np.zeros((max_count_x,alpha_count))\n",
    "    x_t[0][0]=1\n",
    "    for j in range(max_count_x-1):\n",
    "        if j < len(word):\n",
    "            x_t[j+1][alphabets.index(word[j])]=1\n",
    "        else:\n",
    "            x_t[j+1][alpha_count-1]=1\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Takes a word and predicts its phonemes\"\"\"\n",
    "def phoneme_predict(word):\n",
    "    words_x = np.array([convert_to_one_hot(word) for _ in range(batch_size)])\n",
    "    \n",
    "    abcd = sess.run(predicting_logits, {x_train: words_x})[:,:,0][0]\n",
    "    l = []\n",
    "    for k in abcd:\n",
    "        if k==40:\n",
    "            break\n",
    "        l.append(phonemes[k])\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IH', 'N', 'D', 'IY', 'AH']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Testing\"\"\"\n",
    "phoneme_predict(\"INDIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error % = 8.1586230428\n"
     ]
    }
   ],
   "source": [
    "print \"Error % = \" + str(get_error(test_input, test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
