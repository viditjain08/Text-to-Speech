{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ps/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set False for testing\"\"\"\n",
    "training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "108\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading max_frames, speaker_list and count of files\"\"\"\n",
    "with open(\"dataset/max_frames_speakers_count.pkl\") as f:\n",
    "    max_frames, speakers, count = pickle.load(f)\n",
    "    print max_frames\n",
    "    print len(speakers)\n",
    "    print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set of Hyperparameters along with the phoneme list\"\"\"\n",
    "batch_size = 25\n",
    "num_features = 13\n",
    "speak_feats = 16 #features in speaker embedding\n",
    "num_speakers = len(speakers)\n",
    "num_conv_filters = 64\n",
    "num_conv_layers = 4 #layers in convolutional network excluding zeroth layer\n",
    "num_rnn_layers = 3\n",
    "dropout = 0.85\n",
    "num_hidden_gru = 1024\n",
    "beam_width= 20\n",
    "\n",
    "#Phonemes\n",
    "silence = \"SIL\"\n",
    "\n",
    "phonemes = [silence,'AA','AE','AH','AO','AW','AY','B','CH','D','DH','EH',\n",
    "            'ER','EY','F','G','HH','IH','IY','JH','K','L','M','N','NG','OW','OY',\n",
    "            'P','R','S','SH','T','TH','UH','UW','V','W','Y','Z','ZH']\n",
    "phoneme_dict = dict()\n",
    "for i in range(len(phonemes)):\n",
    "    phoneme_dict[phonemes[i]] = i\n",
    "\n",
    "\n",
    "num_phonemes = len(phonemes)\n",
    "num_labels = num_phonemes**2\n",
    "ctc_classes = num_labels + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [batch_size, max_frames,num_features]) ##MFCC features\n",
    "y = tf.sparse_placeholder(tf.int32) ##Phoneme pair labels\n",
    "y_length_inp = tf.placeholder(tf.int32, [batch_size])\n",
    "y_length_pred = tf.placeholder(tf.int32, [batch_size])\n",
    "speaker_selector = tf.placeholder(tf.float32, [batch_size, num_speakers]) #One hot speaker selector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'speaker_embedding/MatMul:0' shape=(25, 16) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"16 feature speaker embedding for better distinction in speakers\"\"\"\n",
    "with tf.variable_scope(\"speaker_embedding\"):\n",
    "    speaker = tf.get_variable(\"speaker\",\n",
    "                              shape=(num_speakers,speak_feats),\n",
    "                              dtype= tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                              trainable=True)\n",
    "    speaker_matrix = tf.matmul(speaker_selector, speaker)\n",
    "speaker_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv_speaker/fully_connected/Softsign:0' shape=(25, 64) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"fully connected layer that takes speaker as the input and sends state to the convolutional layers\"\"\"\n",
    "with tf.variable_scope(\"conv_speaker\"):\n",
    "    conv_speaker_outputs = num_conv_filters\n",
    "    conv_speaker_fc = tf.contrib.layers.fully_connected(speaker_matrix,\n",
    "                                                        conv_speaker_outputs,\n",
    "                                                        activation_fn= tf.nn.softsign)\n",
    "conv_speaker_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn_speaker/fully_connected/Softsign:0' shape=(25, 1024) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"fully connected layer that takes speaker as the input and sends state to the Bidirectional Gru layers\"\"\"\n",
    "with tf.variable_scope(\"rnn_speaker\"):\n",
    "    rnn_speaker_outputs = num_hidden_gru\n",
    "    rnn_speaker = tf.contrib.layers.fully_connected(speaker_matrix,\n",
    "                                                    rnn_speaker_outputs,\n",
    "                                                    activation_fn= tf.nn.softsign)\n",
    "rnn_speaker\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'speaker_state_conv/transpose:0' shape=(25, 500, 13, 64) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Preprocessing of speaker_conv for the convolution layers\"\"\"\n",
    "with tf.variable_scope(\"speaker_state_conv\"):\n",
    "    #finding speaker state for cnn\n",
    "    length = max_frames\n",
    "    breadth = num_features\n",
    "    height = num_conv_filters\n",
    "    depth = batch_size\n",
    "    multiply = length*breadth\n",
    "\n",
    "    ##transforming speaker embedding for convolutional layers\n",
    "    a = tf.expand_dims(conv_speaker_fc , axis = 0)\n",
    "    b = tf.contrib.seq2seq.tile_batch(a, multiplier=multiply)\n",
    "    c = tf.reshape(b,[length,breadth,batch_size,-1]) \n",
    "    speaker_state_conv = tf.transpose(c,[2,0,1,3])\n",
    "    \n",
    "speaker_state_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"zeroth convolutional layer\"\"\"\n",
    "with tf.variable_scope(\"zero_conv_layer\"):\n",
    "    conv0 =  tf.layers.conv2d(tf.expand_dims(x, axis = 3),\n",
    "                              filters = num_conv_filters,\n",
    "                              kernel_size = (9,5),\n",
    "                              padding = \"SAME\")\n",
    "    \n",
    "network = tf.expand_dims(conv0 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stacking convolutional layers with batch normalization and speaker embedding and relu6\"\"\"\n",
    "with tf.variable_scope(\"conv_layers\"):\n",
    "    for i in range(num_conv_layers):\n",
    "        conv = tf.layers.conv2d(network[i],\n",
    "                                filters = num_conv_filters,\n",
    "                                kernel_size = (9,5),\n",
    "                                padding = \"SAME\")\n",
    "\n",
    "        conv = tf.contrib.layers.batch_norm(conv)\n",
    "\n",
    "\n",
    "        #multipying speaker embedding with conv+bn output\n",
    "        conv_layer = tf.multiply(speaker_state_conv, conv)\n",
    "        conv_layer = tf.add(conv_layer, network[i])\n",
    "\n",
    "        #applying relu6\n",
    "        conv_layer = tf.nn.relu6(conv_layer)\n",
    "        \n",
    "        #adding to networks\n",
    "        network = tf.concat([network,tf.expand_dims(conv_layer,0)],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'recurrent_input/Reshape:0' shape=(25, 500, 832) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"setting up dropout layer after convolutions\"\"\"\n",
    "with tf.variable_scope(\"recurrent_input\"):\n",
    "    conv_output = tf.nn.dropout(network[num_conv_layers], keep_prob = dropout)\n",
    "    rnn_input = tf.reshape(conv_output,[batch_size,max_frames,-1])\n",
    "    \n",
    "rnn_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_cell():\n",
    "    return tf.nn.rnn_cell.GRUCell(num_hidden_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn_preprocess/concat:0' shape=(25, 500, 1856) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Making the correct input tensor for the rnn \"\"\"\n",
    "with tf.variable_scope(\"rnn_preprocess\"):\n",
    "    inputs = rnn_input\n",
    "    rnn_input_embed = tf.contrib.seq2seq.tile_batch(tf.expand_dims(rnn_speaker,0), multiplier = max_frames)\n",
    "    rnn_input_embed = tf.transpose(rnn_input_embed,[1,0,2])\n",
    "    inputs = tf.concat([inputs, rnn_input_embed], axis = 2)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_BI3/concat:0' shape=(25, 500, 2048) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Creating the bi directional gru layers\"\"\"\n",
    "for i in range(num_rnn_layers):\n",
    "    with tf.variable_scope(\"RNN_BI\"+str(i+1)):\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(gru_cell(),\n",
    "                                                                 gru_cell(),\n",
    "                                                                 inputs,\n",
    "                                                                 initial_state_fw = rnn_speaker,\n",
    "                                                                 initial_state_bw = rnn_speaker)\n",
    "\n",
    "        inputs = tf.concat(outputs, axis = 2)\n",
    "\n",
    "rnn_output = inputs\n",
    "rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn_postprocess/fully_connected/Reshape_1:0' shape=(25, 500, 1601) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Fully connected layer to convert rnn output to ctc input\"\"\"\n",
    "with tf.variable_scope(\"rnn_postprocess\"):\n",
    "    rnn_output = tf.nn.dropout(rnn_output, keep_prob = dropout)\n",
    " \n",
    "    ctc_input = tf.contrib.layers.fully_connected(rnn_output,\n",
    "                                                  ctc_classes,\n",
    "                                                  activation_fn= tf.nn.softmax)\n",
    "ctc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'CTC_loss/ExpandDims:0' shape=(25, 1) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Calculating CTC loss over the batch\"\"\"\n",
    "with tf.variable_scope(\"CTC_loss\"):\n",
    "    y_true = tf.sparse_tensor_to_dense(y)\n",
    "    ctc_loss = tf.keras.backend.ctc_batch_cost(y_true,\n",
    "                                               ctc_input,\n",
    "                                               input_length = tf.reshape(y_length_pred,[-1,1]),\n",
    "                                               label_length = tf.reshape(y_length_inp,[-1,1]))\n",
    "ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"beam_search/concat:0\", shape=(25, 500, 1601), dtype=bool)\n",
      "Tensor(\"beam_search/Mul:0\", shape=(25, 500, 1601), dtype=float32)\n",
      "TopKV2(values=<tf.Tensor 'beam_search/TopKV2:0' shape=(25, 20) dtype=float32>, indices=<tf.Tensor 'beam_search/TopKV2:1' shape=(25, 20) dtype=int32>)\n",
      "Tensor(\"beam_search/ExpandDims_3:0\", shape=(1, 400, 25), dtype=int32)\n",
      "Tensor(\"beam_search/tile_batch_1/Reshape:0\", shape=(400, 25), dtype=float32)\n",
      "Tensor(\"beam_search/Reshape:0\", shape=(25, 500), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A Custom Built Beam Search For CTC\"\"\"\n",
    "with tf.variable_scope(\"beam_search\"):\n",
    "\n",
    "\n",
    "    batch_const = tf.constant([[j for j in range(batch_size)]])\n",
    "    batch_const = tf.transpose(tf.contrib.seq2seq.tile_batch(batch_const, multiplier=beam_width),[1,0])\n",
    "    \n",
    "    length_filter = tf.tile(tf.expand_dims(y_length_pred,axis=1),[1,max_frames])\n",
    "    temp_mask = tf.constant([[j for j in range(max_frames)]])\n",
    "    temp_mask = tf.tile(temp_mask,[batch_size,1])\n",
    "    frames_mask = tf.less(temp_mask,length_filter)\n",
    "    frames_mask = tf.tile(tf.expand_dims(frames_mask,axis=2),[1,1,ctc_classes-1])\n",
    "    blank_mask = tf.expand_dims(tf.ones([batch_size,max_frames],dtype=tf.float32),axis=2)\n",
    "    blank_mask = tf.cast(blank_mask,tf.bool)\n",
    "    frames_mask = tf.concat([frames_mask,blank_mask],axis=2)\n",
    "    print frames_mask\n",
    "    frames_mask = tf.cast(frames_mask,tf.float32)\n",
    "    ctc_input_masked = tf.multiply(ctc_input,frames_mask)\n",
    "    print ctc_input_masked\n",
    "    ctc_log = tf.log(ctc_input_masked)\n",
    "    frames = tf.unstack(ctc_log,axis=1)\n",
    "    \n",
    "    topk = tf.nn.top_k(frames[0],\n",
    "                       k=beam_width,\n",
    "                       sorted=False)\n",
    "    print topk\n",
    "    values = topk[0]\n",
    "    indices = topk[1]\n",
    "\n",
    "    values = tf.transpose(values, [1,0])\n",
    "    indices = tf.transpose(indices, [1,0])\n",
    "\n",
    "    values = tf.contrib.seq2seq.tile_batch(values, multiplier=beam_width)\n",
    "    indices = tf.contrib.seq2seq.tile_batch(indices, multiplier=beam_width)\n",
    "    indices = tf.expand_dims(indices,axis=0)\n",
    "    print indices\n",
    "    print values\n",
    "\n",
    "\n",
    "    for i in range(1,len(frames)):\n",
    "        topk_next = tf.nn.top_k(frames[i],k=beam_width,sorted=False)\n",
    "        values_next = topk_next[0]\n",
    "        indices_next = topk_next[1]\n",
    "        values_next = tf.transpose(values_next, [1,0])\n",
    "        indices_next = tf.transpose(indices_next, [1,0])\n",
    "        \n",
    "        values_next = tf.tile(values_next, tf.constant([beam_width,1],dtype=tf.int32))\n",
    "        indices_next = tf.tile(indices_next, tf.constant([beam_width,1],dtype=tf.int32))\n",
    "        \n",
    "        indices = tf.concat([indices,tf.expand_dims(indices_next,axis=0)],axis=0)\n",
    "\n",
    "        values = tf.add(values,values_next)\n",
    "        values = tf.transpose(values,[1,0])\n",
    "        tempk = tf.nn.top_k(values,k=beam_width,sorted=False)\n",
    "        values = tf.contrib.seq2seq.tile_batch(tf.transpose(tempk[0],[1,0]), multiplier=beam_width)\n",
    "        indices_new = tf.stack([batch_const,tempk[1]],axis=2)\n",
    "\n",
    "        indices = tf.gather_nd(tf.transpose(indices,[2,1,0]), indices_new)\n",
    "        indices = tf.transpose(indices, [1,0,2])\n",
    "        indices = tf.contrib.seq2seq.tile_batch(indices, multiplier=beam_width)\n",
    "        indices = tf.transpose(indices, [2,0,1])\n",
    "\n",
    "    final_beam = tf.transpose(indices,[2,0,1])\n",
    "    final_beam, _ = tf.split(final_beam,[1,-1],axis=2)\n",
    "    final_path = tf.reshape(final_beam,[batch_size,-1])\n",
    "    print final_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_punctuation(s):\n",
    "        return ''.join(c for c in s if c not in '!\"#$%&\\()*+./<=>?@[\\\\]^_`{|}~')\n",
    "    \n",
    "with open('result.pkl') as f:  \n",
    "    words_phonemes = pickle.load(f)\n",
    "\n",
    "\n",
    "def get_phoneme_pairs(sentence,dict):\n",
    "    \"\"\"Takes a sentence and returns its phoneme pairs including SILENCE at start and end\"\"\"\n",
    "    answer = [silence]\n",
    "    sentence= strip_punctuation(sentence)\n",
    "    words = sentence.replace(':',' ').replace(';',' ').replace(',',' ').split()\n",
    "    \n",
    "    for i in words:\n",
    "        if i[-1] in \",;:\":\n",
    "            if i[:-1]!='':\n",
    "                for j in dict[i[:-1].upper()]:\n",
    "                    answer.append(j)\n",
    "            answer.append(silence)\n",
    "        else:\n",
    "            for j in dict[i.upper()]:\n",
    "                answer.append(j)\n",
    "            answer.append(silence)\n",
    "    answer[-1] = silence\n",
    "    \n",
    "    final_answer = []\n",
    "    for i in range(len(answer)-1):\n",
    "        final_answer.append((answer[i],answer[i+1]))\n",
    "        \n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_labels(phoneme_pairs, label_dict):\n",
    "    \"\"\"Converts Phoneme Pairs to Labels\"\"\"\n",
    "    labels = []\n",
    "    for i in phoneme_pairs:\n",
    "        labels.append(label_dict[i[0]]*(len(label_dict))+label_dict[i[1]])\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_phoneme_pairs(labels, phoneme_list):\n",
    "    \"\"\"Converts labels to Phoneme Pairs\"\"\"\n",
    "    phoneme_pairs = []\n",
    "    for i in labels:\n",
    "        if i ==-1 or i == (ctc_classes - 1):\n",
    "            phoneme_pairs.append(\"-\")\n",
    "        else:\n",
    "            x = i%(len(phoneme_list))\n",
    "            y = i/(len(phoneme_list))\n",
    "            phoneme_pairs.append((phonemes[y],phonemes[x]))\n",
    "    return phoneme_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Learning rate and annealing rate are set for the adam optimizer\"\"\"\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "starter_learning_rate = 2e-4\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, \n",
    "                                           global_step,\n",
    "                                           1000,\n",
    "                                           0.95,\n",
    "                                           staircase=True)\n",
    "\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "minimize = optimizer.minimize(ctc_loss,global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_parameters(labels_batch):\n",
    "    \"\"\"Returns parameters required for a sparse tensor for the phonemes\"\"\"\n",
    "    indices=[]\n",
    "    values=[]\n",
    "    x = len(labels_batch)\n",
    "    max_len = 0\n",
    "    for i in range(len(labels_batch)):\n",
    "        for j in range(len(labels_batch[i])):\n",
    "            indices.append([i,j])\n",
    "        values+=labels_batch[i]\n",
    "        max_len=max(max_len,len(labels_batch[i]))\n",
    "    \n",
    "    return (indices,values,(x,max_len))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_length_inp(labels_batch):\n",
    "    \"\"\"Returns the sequence lengths of input phoneme pairs\"\"\"\n",
    "    y_len = []\n",
    "    for i in range(len(labels_batch)):\n",
    "        y_len.append(len(labels_batch[i]))\n",
    "    return y_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_one_hot(speaker_list, speakers):\n",
    "    \"\"\"Returns speaker one hot encoding for the list\"\"\"\n",
    "    one_hot_speaker = np.zeros((len(speaker_list),len(speakers)))\n",
    "    for i in range(len(speaker_list)):\n",
    "        one_hot_speaker[i][speakers.index(speaker_list[i])]=1\n",
    "    return one_hot_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_dur(ax,y_length_pred_test):\n",
    "    \"\"\"returns the final predicted phonemes along with durations in human readable form\"\"\"\n",
    "    final_dur_list=[]\n",
    "    for i in range(batch_size):\n",
    "        out = ax[i]\n",
    "        dur_list = list()\n",
    "        out = np.insert(out,0,0)\n",
    "        a = labels_to_phoneme_pairs(out[:y_length_pred_test[i]],phonemes)\n",
    "        time_frame=1\n",
    "        prev_pair = a[0]\n",
    "        for phoneme_pairs_a in a:\n",
    "            try:\n",
    "                (b,c)=phoneme_pairs_a\n",
    "                if len(dur_list)!=0 and phoneme_pairs_a==prev_pair:\n",
    "                    time_frame+=1\n",
    "                else:\n",
    "                    dur_list.append([prev_pair[1],time_frame])\n",
    "                    prev_pair = phoneme_pairs_a\n",
    "                    time_frame=1\n",
    "            except:\n",
    "                time_frame+=1\n",
    "        dur_list.append([prev_pair[1],time_frame])\n",
    "        final_dur_list.append(dur_list[2:])\n",
    "        \n",
    "    return final_dur_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_pairs_dur_PPER(ax,y_length_pred_test):\n",
    "    \"\"\"returns the final predicted phonemes along with durations in human readable form for calculating PPER\"\"\"\n",
    "\n",
    "    final_dur_list=[]\n",
    "    for i in range(batch_size):\n",
    "        out = ax[i]\n",
    "        dur_list = list()\n",
    "        out = np.insert(out,0,0)\n",
    "        a = labels_to_phoneme_pairs(out[:y_length_pred_test[i]],phonemes)\n",
    "        time_frame=1\n",
    "        prev_pair = a[0]\n",
    "        for phoneme_pairs_a in a:\n",
    "            try:\n",
    "                (b,c)=phoneme_pairs_a\n",
    "                if len(dur_list)!=0 and phoneme_pairs_a==prev_pair:\n",
    "                    time_frame+=1\n",
    "                else:\n",
    "                    dur_list.append(prev_pair)\n",
    "                    prev_pair = phoneme_pairs_a\n",
    "                    time_frame=1\n",
    "            except:\n",
    "                time_frame+=1\n",
    "        dur_list.append(prev_pair)\n",
    "        final_dur_list.append(dur_list[2:])\n",
    "        \n",
    "    return final_dur_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(true,pred):\n",
    "    \"\"\"Comparison algorithm for true and predicted phoneme pairs\"\"\"\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    incorrect=0\n",
    "    window_size=5\n",
    "    total = 0\n",
    "    for i in range(len(true)):\n",
    "        if ptr1>len(true)-1 or ptr2>len(pred)-1:\n",
    "            break\n",
    "        if true[ptr1]==pred[ptr2]:\n",
    "            ptr1+=1\n",
    "            ptr2+=1\n",
    "            total += 1\n",
    "        else:\n",
    "            try:\n",
    "                jump = pred[ptr2:min(ptr2+window_size,len(pred))].index(true[ptr1]) \n",
    "                incorrect += jump\n",
    "                total+=jump\n",
    "                ptr2+=jump\n",
    "            except:\n",
    "                ptr1+=1\n",
    "                incorrect += 1\n",
    "                total+=1\n",
    "    return total,incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPER(test_data):\n",
    "    \"\"\"returns phoneme pair error rate\"\"\"\n",
    "    no_of_batches=len(test_data)/batch_size\n",
    "    ptr=0\n",
    "    incorrect_pairs=0\n",
    "    total_pairs=0\n",
    "    for k in range(no_of_batches):        \n",
    "        inp = test_data[ptr:ptr+batch_size]\n",
    "        inp = zip(*inp)\n",
    "        x_test = np.asarray(inp[2])\n",
    "        temp = inp[1]\n",
    "        y_labels = []\n",
    "        for sentence in temp:\n",
    "            y_labels.append(convert_to_labels(get_phoneme_pairs(sentence,words_phonemes),phoneme_dict))\n",
    "        y_test = get_sparse_parameters(y_labels)\n",
    "        y_length_inp_test = get_y_length_inp(y_labels)\n",
    "        #print 'input', y_length_inp_train\n",
    "        y_length_pred_test = list(inp[3])\n",
    "        #print \"pred\", y_length_pred_train\n",
    "        speaker_selector_test = get_speaker_one_hot(inp[0], speakers)\n",
    "        ptr+=batch_size\n",
    "        ax = sess.run(final_path,{x:x_test, y:y_test, y_length_inp:y_length_inp_test,y_length_pred:y_length_pred_test,\n",
    "                                                 speaker_selector:speaker_selector_test})\n",
    "        if not k:\n",
    "            print labels_to_phoneme_pairs(ax[0][:200],phonemes)\n",
    "        pair_dur = pred_pairs_dur_PPER(ax,y_length_pred_test)\n",
    "        for i in range(batch_size):\n",
    "            p_pairs = labels_to_phoneme_pairs(y_labels[i],phonemes)\n",
    "            p_pairs_pred = pair_dur[i]\n",
    "            total,incorrect = compare(p_pairs, p_pairs_pred)\n",
    "            total_pairs+=total\n",
    "            incorrect_pairs+=incorrect\n",
    "    return (float(incorrect_pairs*100)/total_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize session\"\"\"\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_models/epoch10/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For using previous models, restore, else comment\"\"\"\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"saved_models/epoch10/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.2 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"For Training only- set number of epochs. Calculates error% at every epoch\"\"\"\n",
    "if training:\n",
    "    epoch = 1\n",
    "    cur_error=0\n",
    "    min_error=100\n",
    "    with open(\"dataset/1.pkl\") as f:\n",
    "                data = pickle.load(f)\n",
    "    for i in range(epoch):\n",
    "        for j in range(1):\n",
    "    #         with open(\"dataset/\"+str(j+1)+\".pkl\") as f:\n",
    "    #             data = pickle.load(f)\n",
    "            train_data, test_data, _, _ = train_test_split(data,data, test_size=0.1, random_state=42)\n",
    "            no_of_batches=len(train_data)/batch_size\n",
    "            ptr=0\n",
    "            for k in range(no_of_batches):\n",
    "                if k%100==0:\n",
    "                    print k\n",
    "                inp = train_data[ptr:ptr+batch_size]\n",
    "                inp = zip(*inp)\n",
    "                x_train = np.asarray(inp[2])\n",
    "                temp = inp[1]\n",
    "                y_labels = []\n",
    "                for sentence in temp:\n",
    "                    y_labels.append(convert_to_labels(get_phoneme_pairs(sentence,words_phonemes),phoneme_dict))\n",
    "                y_train = get_sparse_parameters(y_labels)\n",
    "                y_length_inp_train = get_y_length_inp(y_labels)\n",
    "                y_length_pred_train = list(inp[3])\n",
    "                speaker_selector_train = get_speaker_one_hot(inp[0], speakers)\n",
    "                ptr+=batch_size\n",
    "\n",
    "                sess.run(minimize,{x:x_train, y:y_train, y_length_inp:y_length_inp_train,y_length_pred:y_length_pred_train,\n",
    "                                   speaker_selector:speaker_selector_train})\n",
    "        print \"Epoch - \",str(i)\n",
    "        cur_error = PPER(test_data[:100])\n",
    "        print \"ERROR- \",str(cur_error)\n",
    "        if cur_error < min_error:\n",
    "            save_path = saver.save(sess,\"saved_models/epoch\"+str(i)+\"/model.ckpt\")\n",
    "            print \"Saved @ \", save_path\n",
    "            min_error = cur_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predicts and stores durations to be used in duration prediction model\"\"\"\n",
    "durations = []\n",
    "speaker_dur = []\n",
    "file_path = []\n",
    "for j in range(1):\n",
    "    with open(\"dataset/\"+str(j+1)+\".pkl\") as f:\n",
    "        data = pickle.load(f)\n",
    "    train_data, test_data, _, _ = train_test_split(data,data, test_size=0.1, random_state=42)\n",
    "    no_of_batches=len(train_data)/batch_size\n",
    "    ptr=0\n",
    "    for k in range(no_of_batches):\n",
    "        print k\n",
    "        inp = train_data[ptr:ptr+batch_size]\n",
    "        inp = zip(*inp)\n",
    "        x_train = np.asarray(inp[2])\n",
    "        temp = inp[1]\n",
    "        y_labels = []\n",
    "        for sentence in temp:\n",
    "            y_labels.append(convert_to_labels(get_phoneme_pairs(sentence,words_phonemes),phoneme_dict))\n",
    "        y_train = get_sparse_parameters(y_labels)\n",
    "        y_length_inp_train = get_y_length_inp(y_labels)\n",
    "        #print 'input', y_length_inp_train\n",
    "        y_length_pred_train = list(inp[3])\n",
    "        #print \"pred\", y_length_pred_train\n",
    "        speaker_selector_train = get_speaker_one_hot(inp[0], speakers)\n",
    "        ptr+=batch_size\n",
    "        durations += pred_dur(sess.run(final_path,{x:x_train, y:y_train, y_length_inp:y_length_inp_train,y_length_pred:y_length_pred_train,\n",
    "                                     speaker_selector:speaker_selector_train}),y_length_pred_train)\n",
    "        speaker_dur += list(inp[0])\n",
    "        file_path += list(inp[4])\n",
    "print len(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Pickles durations of phonemes of sentences\"\"\"\n",
    "speaker_info = []\n",
    "speaker_info.append(speaker_dur)\n",
    "speaker_info.append(file_path)\n",
    "speaker_info = zip(*speaker_info)\n",
    "print speaker_info[:10]\n",
    "for i in  range(len(speaker_info)):\n",
    "    speaker_info[i] = list(speaker_info[i])\n",
    "print speaker_info[:10]\n",
    "with open(\"duration_pred.pkl\",'w') as f:\n",
    "    pickle.dump([durations,speaker_info],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SIL', 'DH'), ('DH', 'AH'), ('AH', 'SIL'), ('AH', 'SIL'), '-', '-', '-', '-', '-', '-', ('SIL', 'M'), ('M', 'AE'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('AE', 'N'), ('N', 'SIL'), '-', '-', '-', '-', '-', ('SIL', 'W'), ('W', 'AA'), ('W', 'AA'), '-', '-', ('AA', 'Z'), ('Z', 'SIL'), ('Z', 'SIL'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('SIL', 'P'), ('SIL', 'P'), ('P', 'R'), ('R', 'AH'), '-', '-', '-', ('AH', 'N'), ('AH', 'N'), '-', '-', '-', '-', ('N', 'AW'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('AW', 'N'), ('N', 'S'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('S', 'T'), ('S', 'T'), ('T', 'SIL'), '-', '-', '-', ('SIL', 'D'), ('D', 'EH'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('EH', 'D'), ('D', 'SIL'), '-', '-', '-', ('SIL', 'AA'), ('SIL', 'AA'), '-', '-', '-', '-', ('AA', 'N'), ('N', 'SIL'), ('N', 'SIL'), '-', '-', '-', '-', ('SIL', 'ER'), ('SIL', 'ER'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('ER', 'AY'), ('ER', 'AY'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('AY', 'V'), '-', '-', '-', '-', '-', '-', '-', '-', ('V', 'AH'), ('V', 'AH'), ('AH', 'L'), ('AH', 'L'), '-', '-', '-', '-', '-', '-', ('L', 'SIL'), ('L', 'SIL'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
      "1.16707616708\n",
      "[('SIL', 'HH'), ('SIL', 'AE'), '-', '-', ('AE', 'Z'), ('Z', 'SIL'), '-', '-', '-', '-', '-', '-', '-', '-', '-', ('SIL', 'DH'), ('DH', 'AH'), '-', '-', ('AH', 'SIL'), '-', '-', '-', '-', '-', ('SIL', 'K'), ('SIL', 'K'), ('K', 'AA'), '-', '-', '-', '-', '-', '-', '-', ('AA', 'N'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('N', 'F'), ('N', 'F'), ('F', 'L'), ('F', 'L'), ('F', 'L'), '-', ('L', 'IH'), '-', '-', '-', '-', '-', ('IH', 'K'), '-', '-', '-', '-', '-', ('T', 'SIL'), ('T', 'SIL'), ('T', 'SIL'), ('SIL', 'S'), ('SIL', 'S'), ('SIL', 'S'), '-', '-', '-', ('S', 'T'), ('S', 'T'), ('S', 'T'), '-', '-', '-', '-', ('T', 'AA'), ('AA', 'R'), ('AA', 'R'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('R', 'T'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('T', 'AH'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', ('AH', 'D'), ('D', 'SIL'), ('D', 'SIL'), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
      "17.2343522562\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/1.pkl\") as f:\n",
    "    data = pickle.load(f)\n",
    "train_data, test_data, _, _ = train_test_split(data,data, test_size=0.1, random_state=42)\n",
    "\n",
    "print PPER(train_data[:100])\n",
    "print PPER(test_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
